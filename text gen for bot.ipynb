{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a87f09f",
   "metadata": {
    "cellId": "httnrqq9kambz3f1be35xm"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import string\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "import annoy\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "import gensim.downloader as api\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56f71204",
   "metadata": {
    "cellId": "znjkrf3x60qe68738xglkb"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "s1 = pd.read_csv('House MD/season1.csv', encoding= 'unicode_escape')\n",
    "s2 = pd.read_csv('House MD/season2.csv', encoding= 'unicode_escape')\n",
    "s3 = pd.read_csv('House MD/season3.csv', encoding= 'unicode_escape')\n",
    "s4 = pd.read_csv('House MD/season4.csv', encoding= 'unicode_escape')\n",
    "s5 = pd.read_csv('House MD/season5.csv', encoding= 'unicode_escape')\n",
    "s6 = pd.read_csv('House MD/season6.csv', encoding= 'unicode_escape')\n",
    "s7 = pd.read_csv('House MD/season7.csv', encoding= 'unicode_escape')\n",
    "s8 = pd.read_csv('House MD/season8.csv', encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf8cbbc4",
   "metadata": {
    "cellId": "w0qm2h1wb42xqqdpf6ww5"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "all_transcripts = [s1, s2, s3, s4, s5, s6, s7, s8]\n",
    "\n",
    "house_md_df = pd.concat(all_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ffbeede3",
   "metadata": {
    "cellId": "dswsbmhf6bzt1kgxwd069"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melanie</td>\n",
       "      <td>Why are you late?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rebecca</td>\n",
       "      <td>You're not going to like the answer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Melanie</td>\n",
       "      <td>I already know the answer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rebecca</td>\n",
       "      <td>I missed the bus.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Melanie</td>\n",
       "      <td>I don't doubt it, no bus stops near Brad's. Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8034</th>\n",
       "      <td>House</td>\n",
       "      <td>Just switched the dental records.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8035</th>\n",
       "      <td>Wilson</td>\n",
       "      <td>You're destroying your entire life. You can't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8036</th>\n",
       "      <td>House</td>\n",
       "      <td>I'm dead, Wilson. How do you want to spend yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8037</th>\n",
       "      <td>Wilson</td>\n",
       "      <td>When the cancer starts getting really bad!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8038</th>\n",
       "      <td>House</td>\n",
       "      <td>Cancer's boring.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75312 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         name                                               line\n",
       "0     Melanie                                  Why are you late?\n",
       "1     Rebecca               You're not going to like the answer.\n",
       "2     Melanie                         I already know the answer.\n",
       "3     Rebecca                                  I missed the bus.\n",
       "4     Melanie   I don't doubt it, no bus stops near Brad's. Y...\n",
       "...       ...                                                ...\n",
       "8034    House                  Just switched the dental records.\n",
       "8035   Wilson   You're destroying your entire life. You can't...\n",
       "8036    House   I'm dead, Wilson. How do you want to spend yo...\n",
       "8037   Wilson         When the cancer starts getting really bad!\n",
       "8038    House                                   Cancer's boring.\n",
       "\n",
       "[75312 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "house_md_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b4afe34",
   "metadata": {
    "cellId": "kat5k6ceiscgzqofkcf5v"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "morpher = MorphAnalyzer()\n",
    "sw = set(get_stop_words(\"en\"))\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2ea17d5",
   "metadata": {
    "cellId": "tk3swp90xzs431kw0ytcwk"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "corpus = []\n",
    "for line in house_md_df.line:\n",
    "    if isinstance(line, str):\n",
    "        corpus.append(line)\n",
    "    \n",
    "corpus = ''.join(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5529f588",
   "metadata": {
    "cellId": "bmbtf13n7u9wyecx0euvmn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class CharDataset:\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "\n",
    "        self.tensors = []\n",
    "        for n in range(self.__len__()):\n",
    "            i = n * self.block_size\n",
    "            chunk = self.data[i:i+self.block_size+1]\n",
    "            dix = [self.stoi[s] for s in chunk]\n",
    "            x = tf.convert_to_tensor(dix[:-1], dtype=tf.int32)\n",
    "            y = tf.convert_to_tensor(dix[1:], dtype=tf.int32)\n",
    "            self.tensors.append((x,y))\n",
    "            \n",
    "    def convert_text(self, text):\n",
    "        dix = [self.stoi[s] for s in text]\n",
    "\n",
    "        return tf.convert_to_tensor(dix, dtype=tf.int32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / (self.block_size + 1))\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x,y in self.tensors:\n",
    "          yield x, y\n",
    "        \n",
    "    __call__ = __iter__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b394f71c",
   "metadata": {
    "cellId": "gihvwxhh3nu8ykgua0ggoe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 5248653 characters, 148 unique.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 13:06:56.567054: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-06 13:06:59.796748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30995 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:8c:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "block_size = 128 \n",
    "train_dataset_gen = CharDataset(corpus, block_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62a31ad5",
   "metadata": {
    "cellId": "bvf6n72ce6jujtgmzv0s"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_dataset = tf.data.Dataset.from_generator(train_dataset_gen,\n",
    "  output_signature=(\n",
    "         tf.TensorSpec(shape=(block_size), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(block_size), dtype=tf.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e300ef40",
   "metadata": {
    "cellId": "tl41l9qjkuhymkjajsofvo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "768ae94d",
   "metadata": {
    "cellId": "gr8yg1tv9g9dndzcg26md"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Length of the vocabulary in chars\n",
    "vocab = sorted(set(corpus))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 128\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6cc2f64f",
   "metadata": {
    "cellId": "t5sl6rxqdeg5etoy9xtjq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "                                 \n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "         tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        \n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "                                   \n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da54ec1d",
   "metadata": {
    "cellId": "ast3nmjqbueapgttrkpo7e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_lstm = build_model_lstm(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86ad31ae",
   "metadata": {
    "cellId": "nn6t5h7byovyspx3musd"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04d7d5e9",
   "metadata": {
    "cellId": "5v6riu8bmtmw0scvxecvap"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model_lstm.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "482ec8ed",
   "metadata": {
    "cellId": "0ulpyqs3a5mook105cyexjp"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_freq=88*3,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "80d94bc8",
   "metadata": {
    "cellId": "wbw9r0ici3p7w5bijlsf5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "635/635 [==============================] - 127s 197ms/step - loss: 0.6161\n",
      "Epoch 2/10\n",
      "635/635 [==============================] - 132s 205ms/step - loss: 0.6179\n",
      "Epoch 3/10\n",
      "635/635 [==============================] - 128s 199ms/step - loss: 0.6175\n",
      "Epoch 4/10\n",
      "635/635 [==============================] - 132s 206ms/step - loss: 0.6186\n",
      "Epoch 5/10\n",
      "635/635 [==============================] - 127s 197ms/step - loss: 0.6172\n",
      "Epoch 6/10\n",
      "635/635 [==============================] - 127s 198ms/step - loss: 0.6191\n",
      "Epoch 7/10\n",
      "635/635 [==============================] - 132s 205ms/step - loss: 0.6211\n",
      "Epoch 8/10\n",
      "635/635 [==============================] - 127s 198ms/step - loss: 0.6207\n",
      "Epoch 9/10\n",
      "635/635 [==============================] - 133s 207ms/step - loss: 0.6222\n",
      "Epoch 10/10\n",
      "635/635 [==============================] - 128s 199ms/step - loss: 0.6237\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "EPOCHS = 10\n",
    "history = model_lstm.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "model_lstm.save_weights(\"bot_lstm_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "53e669d5",
   "metadata": {
    "cellId": "o71jx7x9mcf1b55psj9bf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)#!g1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6e991b19",
   "metadata": {
    "cellId": "bwkqusug6zr40zu32ycnsx"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def generate_text2(model, start_string):\n",
    "    input_eval = train_dataset_gen.convert_text(start_string)\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "#     num_generate = 50\n",
    "    \n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 0.5\n",
    "\n",
    "    model.reset_states()\n",
    "    last_char = ''\n",
    "    while last_char not in ['!', '.', '?']:\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        last_char = train_dataset_gen.itos[predicted_id]\n",
    "        text_generated.append(last_char)\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5b422991",
   "metadata": {
    "cellId": "c3bozb3sgxh702xm8el5q4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's not Lupus! Well, if you want to get the lique in front of the TV?\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "model_lstm_pred = build_model_lstm(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=1)\n",
    "\n",
    "model_lstm_pred.load_weights(\"bot_lstm_weights\").expect_partial()\n",
    "\n",
    "text_ = generate_text2(model_lstm_pred, start_string=u\"It's not Lupus!\")\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7876f9de",
   "metadata": {
    "cellId": "1vt5eeexdh9hx6qaueuqyg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1865e8",
   "metadata": {
    "cellId": "x4sdy68hdal1w9j1g28i"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "10dca30e-559b-40ee-b293-20842c40f523",
  "notebookPath": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
