{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccuoYcScfRNe",
    "outputId": "ddb22472-085a-4833-cccc-e32cf50ff0a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting stop_words\n",
      "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: stop_words, docopt\n",
      "  Building wheel for stop_words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for stop_words: filename=stop_words-2018.7.23-py3-none-any.whl size=32910 sha256=8d7e3397b8bcb90367568da72aee52d468f54b70291d39cfd00a608fb01d180d\n",
      "  Stored in directory: /root/.cache/pip/wheels/d0/1a/23/f12552a50cb09bcc1694a5ebb6c2cd5f2a0311de2b8c3d9a89\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=d71d72100ac78eb31459eeb7d6be117464733b3738387c20c8be29d8fc451c6a\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built stop_words docopt\n",
      "Installing collected packages: stop_words, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 stop_words-2018.7.23\n"
     ]
    }
   ],
   "source": [
    "# pip install stop_words nltk pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "NZBXuMindqRb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input, Embedding, Conv1D, GlobalMaxPool1D\n",
    "from keras.layers import SimpleRNN, LSTM, GRU, Masking, MaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from keras.callbacks import TensorBoard \n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy \n",
    "from keras.callbacks import EarlyStopping  \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0OT3CqdVnWQz",
    "outputId": "c697f145-c472-4ea4-a629-e7ad1f18f24a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]     C:\\Users\\Alina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_ru is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "unDK3_roe4uB"
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_excel('–æ—Ç–∑—ã–≤—ã –∑–∞ –ª–µ—Ç–æ.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UQ8NAzDUp9Ji"
   },
   "outputs": [],
   "source": [
    "reviews.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WAXnOy0ryX7U"
   },
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(txt):\n",
    "  txt = str(txt)\n",
    "  txt = txt.lower()\n",
    "  txt = re.sub('[0-9]|[-‚Äî.,:;_%¬©¬´¬ª?*!@#‚Ññ$^‚Ä¢¬∑\"&()]|[+=]|[[]|[]]|[/]|[..]|[...]|[....]|[d]', ' ', txt)\n",
    "  txt = [morph.parse(word)[0].normal_form for word in txt.split() if word not in stopwords.words('russian')]\n",
    "  # txt = \" \".join(txt)\n",
    "  return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eeeiJMT-y5eb",
    "outputId": "d7f85483-fd72-49bb-d0cc-a9aa865c1c0b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-9a365ce90dba>:6: FutureWarning: Possible nested set at position 40\n",
      "  txt = re.sub('[0-9]|[-‚Äî.,:;_%¬©¬´¬ª?*!@#‚Ññ$^‚Ä¢¬∑\"&()]|[+=]|[[]|[]]|[/]|[..]|[...]|[....]|[d]', ' ', txt)\n"
     ]
    }
   ],
   "source": [
    "reviews['clean_text'] = reviews.Content.apply(preprocess_text)\n",
    "reviews['label'] = reviews.Rating.map({5:0, 4:0, 3:1, 2:1, 1:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "yyBb-kQa9mlI",
    "outputId": "269c1a25-1ccb-4dc3-936b-29d67424d01b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Content</th>\n",
       "      <th>Date</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>It just works!</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>[it, just, works]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>–í —Ü–µ–ª–æ–º —É–¥–æ–±–Ω–æ–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ...–∏–∑ –º–∏–Ω—É—Å–æ–≤ —Ö–æ—Ç—è—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –¥–æ—Å—Ç—É–ø –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º –≤ —Ç–µ–ª–µ—Ñ–æ–Ω–µ,–ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>[—Ü–µ–ª–æ–µ, —É–¥–æ–±–Ω–æ–Ω–æ–π, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, –º–∏–Ω—É—Å, —Ö–æ—Ç–µ—Ç—å, —Å–ª–∏—à–∫–æ–º, –±–æ–ª—å—à–æ–π, –¥–æ—Å—Ç—É–ø, –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π, –¥–∞–Ω–Ω—ã–µ, —Ç–µ–ª–µ—Ñ–æ–Ω, –ø—Ä–∏—Ö–æ–¥–∏—Ç—å—Å—è, –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π, —Ä–µ–∂–∏–º]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>–û—Ç–ª–∏—á–Ω–æ –≤—Å–µ</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>[–æ—Ç–ª–∏—á–Ω–æ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>–°—Ç–∞–ª –∑–∞–≤–∏—Å–∞—Ç—å –Ω–∞ 1% —Ä–∞–±–æ—Ç—ã –∞–Ω—Ç–∏–≤–∏—Ä—É—Å–∞. –î–∞–ª—å—à–µ –Ω–∏–∫—É–¥–∞. –†–∞–Ω–µ–µ –±–æ–ª—å—à–µ –≥–æ–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ.</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>[—Å—Ç–∞—Ç—å, –∑–∞–≤–∏—Å–∞—Ç—å, —Ä–∞–±–æ—Ç–∞, –∞–Ω—Ç–∏–≤–∏—Ä—É—Å, –¥–∞–ª—ë–∫–∏–π, –Ω–∏–∫—É–¥–∞, —Ä–∞–Ω–µ–µ, –≥–æ–¥, –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è, –Ω–æ—Ä–º–∞–ª—å–Ω–æ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>–û—á–µ–Ω—å —É–¥–æ–±–Ω–æ, —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–æ.</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>[–æ—á–µ–Ω—å, —É–¥–æ–±–Ω–æ, —Ä–∞–±–æ—Ç–∞—Ç—å, –±—ã—Å—Ç—Ä–æ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>–í—Å—ë —É–¥–æ–±–Ω–æ –Ω–æ—Ä–º üëçüëçüëç</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>[–≤—Å—ë, —É–¥–æ–±–Ω–æ, –Ω–æ—Ä–º–∞, üëçüëçüëç]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>–û—á–µ–Ω—å —É–¥–æ–±–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ.</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>[–æ—á–µ–Ω—å, —É–¥–æ–±–Ω—ã–π, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                                                                                                                                Content        Date                                                                                                                                              clean_text  label\n",
       "0       5                                                                                                                                         It just works!  2017-08-14                                                                                                                                       [it, just, works]      0\n",
       "1       4  –í —Ü–µ–ª–æ–º —É–¥–æ–±–Ω–æ–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ...–∏–∑ –º–∏–Ω—É—Å–æ–≤ —Ö–æ—Ç—è—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –¥–æ—Å—Ç—É–ø –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º –≤ —Ç–µ–ª–µ—Ñ–æ–Ω–µ,–ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ  2017-08-14  [—Ü–µ–ª–æ–µ, —É–¥–æ–±–Ω–æ–Ω–æ–π, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, –º–∏–Ω—É—Å, —Ö–æ—Ç–µ—Ç—å, —Å–ª–∏—à–∫–æ–º, –±–æ–ª—å—à–æ–π, –¥–æ—Å—Ç—É–ø, –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π, –¥–∞–Ω–Ω—ã–µ, —Ç–µ–ª–µ—Ñ–æ–Ω, –ø—Ä–∏—Ö–æ–¥–∏—Ç—å—Å—è, –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π, —Ä–µ–∂–∏–º]      0\n",
       "2       5                                                                                                                                            –û—Ç–ª–∏—á–Ω–æ –≤—Å–µ  2017-08-14                                                                                                                                               [–æ—Ç–ª–∏—á–Ω–æ]      0\n",
       "3       5                                                         –°—Ç–∞–ª –∑–∞–≤–∏—Å–∞—Ç—å –Ω–∞ 1% —Ä–∞–±–æ—Ç—ã –∞–Ω—Ç–∏–≤–∏—Ä—É—Å–∞. –î–∞–ª—å—à–µ –Ω–∏–∫—É–¥–∞. –†–∞–Ω–µ–µ –±–æ–ª—å—à–µ –≥–æ–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ.  2017-08-14                                                              [—Å—Ç–∞—Ç—å, –∑–∞–≤–∏—Å–∞—Ç—å, —Ä–∞–±–æ—Ç–∞, –∞–Ω—Ç–∏–≤–∏—Ä—É—Å, –¥–∞–ª—ë–∫–∏–π, –Ω–∏–∫—É–¥–∞, —Ä–∞–Ω–µ–µ, –≥–æ–¥, –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è, –Ω–æ—Ä–º–∞–ª—å–Ω–æ]      0\n",
       "4       5                                                                                                                         –û—á–µ–Ω—å —É–¥–æ–±–Ω–æ, —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–æ.  2017-08-14                                                                                                                       [–æ—á–µ–Ω—å, —É–¥–æ–±–Ω–æ, —Ä–∞–±–æ—Ç–∞—Ç—å, –±—ã—Å—Ç—Ä–æ]      0\n",
       "5       5                                                                                                                                    –í—Å—ë —É–¥–æ–±–Ω–æ –Ω–æ—Ä–º üëçüëçüëç  2017-08-14                                                                                                                               [–≤—Å—ë, —É–¥–æ–±–Ω–æ, –Ω–æ—Ä–º–∞, üëçüëçüëç]      0\n",
       "6       5                                                                                                                              –û—á–µ–Ω—å —É–¥–æ–±–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ.  2017-08-14                                                                                                                            [–æ—á–µ–Ω—å, —É–¥–æ–±–Ω—ã–π, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ]      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "foTeWNLU4yXN"
   },
   "outputs": [],
   "source": [
    "train_df, valid_df = model_selection.train_test_split(reviews, random_state=678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBsmPqHUJXsY",
    "outputId": "caf41bdf-7795-4777-ef8b-7daa2b56e845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87434\n"
     ]
    }
   ],
   "source": [
    "full_corpus = []\n",
    "\n",
    "for text in train_df.clean_text:\n",
    "  for word in text:\n",
    "    full_corpus.append(word)\n",
    "\n",
    "print(len(full_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LDou3RgvMCmB",
    "outputId": "71709c22-c931-4ba0-a845-f983799ac938"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7354"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dict = Counter(full_corpus)\n",
    "freq_dict_sorted= sorted(freq_dict.items(), key=lambda x: -x[1])\n",
    "len(freq_dict_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grkjL1EQMRZO",
    "outputId": "40240d26-a1e7-4bdb-f2d1-2ddd9839b6d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137, 877, 6340)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_freq_words = []\n",
    "mid_freq_words = []\n",
    "low_freq_words = []\n",
    "\n",
    "for word, freq in freq_dict_sorted:\n",
    "  if freq >= 100:\n",
    "    high_freq_words.append(word)\n",
    "  elif freq < 100 and freq >= 10:\n",
    "    mid_freq_words.append(word)\n",
    "  else: \n",
    "    low_freq_words.append(word)\n",
    "\n",
    "len(high_freq_words), len(mid_freq_words), len(low_freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHzMyKZqbvpL",
    "outputId": "0c53c30f-8508-4153-a5a3-d8a8204404fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1014"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = high_freq_words + mid_freq_words\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "naglIxcOoNOA",
    "outputId": "6e94a8a2-0091-4067-b1e3-01a665d84059"
   },
   "outputs": [],
   "source": [
    "vec1 = CountVectorizer(lowercase=False, analyzer=lambda x: x, ngram_range=(1, 4), vocabulary=vocab)\n",
    "x_train_cv = vec1.fit_transform(train_df.clean_text).toarray()\n",
    "x_valid_cv = vec1.fit_transform(valid_df.clean_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Dq-0Ak7SPnAD"
   },
   "outputs": [],
   "source": [
    "vec2 = TfidfVectorizer(lowercase=False, analyzer=lambda x: x, ngram_range=(1, 4), vocabulary=vocab)\n",
    "x_train_tfidf = vec2.fit_transform(train_df.clean_text).toarray()\n",
    "x_valid_tfidf = vec2.fit_transform(valid_df.clean_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "qfEKg3Ek_o5I"
   },
   "outputs": [],
   "source": [
    "max_words = len(freq_dict_sorted)\n",
    "max_len = len(vocab)\n",
    "\n",
    "# Training\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "print_batch_n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jIdicJsA7rI",
    "outputId": "ae2b05eb-ba0c-4e0a-c328-b3da4c0c0eb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15492, 1014), (15492, 1014))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_cv.shape, x_train_tfidf.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k6OmyRYPVfik",
    "outputId": "cdac945c-c4db-4c28-dec4-828d2f7e5d59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5164, 1014), (5164, 1014))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_cv.shape, x_valid_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "i1WghmzqBcPs"
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(train_df.label, num_classes)\n",
    "y_valid = keras.utils.to_categorical(valid_df.label, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2ZCcC2r9FqPe"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
    "model.add(Conv1D(128, 3))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DwTrVXLnFzgT"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='CategoricalCrossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XngCit1MF5zf",
    "outputId": "af30c5fe-afd2-4d14-8bdd-e0c43e87c104"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "109/109 [==============================] - 22s 117ms/step - loss: 0.4635 - accuracy: 0.8183 - val_loss: 0.4358 - val_accuracy: 0.8310\n",
      "Epoch 2/20\n",
      "109/109 [==============================] - 12s 112ms/step - loss: 0.4363 - accuracy: 0.8281 - val_loss: 0.4381 - val_accuracy: 0.8323\n"
     ]
    }
   ],
   "source": [
    "tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "history = model.fit(x_train_cv, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[tensorboard, early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPtqblXPGFjg",
    "outputId": "80227edb-95da-473d-fbc6-6750ad39ad84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 2s 48ms/step - loss: 0.4547 - accuracy: 0.8292\n",
      "\n",
      "\n",
      "Test score: 0.45471957325935364\n",
      "Test accuracy: 0.8292021751403809\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_valid_cv, y_valid, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Jv46nwjYYutB"
   },
   "outputs": [],
   "source": [
    "y_train_rnn = train_df.label.values\n",
    "y_valid_rnn = valid_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fkjCIBMh6urn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(input_dim=max_words, output_dim=30, input_length=max_len, trainable=True, mask_zero=True))\n",
    "model_lstm.add(Masking(mask_value=0.0))\n",
    "model_lstm.add(LSTM(64, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(64, activation='relu'))\n",
    "model_lstm.add(Dropout(0.5))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2T-2L3Id7qyh"
   },
   "outputs": [],
   "source": [
    "model_lstm.compile(loss='BinaryCrossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zym3iaQ379Av",
    "outputId": "5b20bdcb-6102-44a2-c7b0-89233cc122ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "109/109 [==============================] - 830s 8s/step - loss: 0.5073 - accuracy: 0.8116 - val_loss: 0.4822 - val_accuracy: 0.8142\n",
      "Epoch 2/20\n",
      "109/109 [==============================] - 789s 7s/step - loss: 0.4911 - accuracy: 0.8127 - val_loss: 0.4820 - val_accuracy: 0.8142\n"
     ]
    }
   ],
   "source": [
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "history = model_lstm.fit(x_train_cv, y_train_rnn,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "QNQFtx8H75dI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 28s 676ms/step - loss: 0.5067 - accuracy: 0.7994\n",
      "\n",
      "\n",
      "Test score: 0.506669819355011\n",
      "Test accuracy: 0.7993803024291992\n"
     ]
    }
   ],
   "source": [
    "score = model_lstm.evaluate(x_valid_cv, y_valid_rnn, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model_gru = Sequential()\n",
    "model_gru.add(Embedding(input_dim=max_words, output_dim=30, input_length=max_len, trainable=True, mask_zero=True))\n",
    "model_gru.add(Masking(mask_value=0.0))\n",
    "model_gru.add(GRU(64, recurrent_dropout=0.2))\n",
    "model_gru.add(Dense(64, activation='relu'))\n",
    "model_gru.add(Dropout(0.5))\n",
    "model_gru.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.compile(loss='BinaryCrossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "109/109 [==============================] - 730s 7s/step - loss: 0.5113 - accuracy: 0.8120 - val_loss: 0.4800 - val_accuracy: 0.8142\n",
      "Epoch 2/20\n",
      "109/109 [==============================] - 715s 7s/step - loss: 0.4904 - accuracy: 0.8127 - val_loss: 0.4816 - val_accuracy: 0.8142\n"
     ]
    }
   ],
   "source": [
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "history = model_gru.fit(x_train_cv, y_train_rnn,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 26s 633ms/step - loss: 0.5014 - accuracy: 0.7994\n",
      "\n",
      "\n",
      "Test score: 0.5014299750328064\n",
      "Test accuracy: 0.7993803024291992\n"
     ]
    }
   ],
   "source": [
    "score = model_gru.evaluate(x_valid_cv, y_valid_rnn, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comb = Sequential()\n",
    "model_comb.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
    "model_comb.add(Conv1D(128, 3))\n",
    "model_comb.add(Activation(\"relu\"))\n",
    "model_comb.add(MaxPooling1D(pool_size=2))\n",
    "model_comb.add(LSTM(100))\n",
    "# model_comb.add(Dropout(0.2))\n",
    "model_comb.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comb.compile(loss='BinaryCrossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "109/109 [==============================] - 25s 206ms/step - loss: 0.4935 - accuracy: 0.8063 - val_loss: 0.4818 - val_accuracy: 0.8142\n",
      "Epoch 2/20\n",
      "109/109 [==============================] - 22s 201ms/step - loss: 0.4823 - accuracy: 0.8127 - val_loss: 0.4760 - val_accuracy: 0.8155\n"
     ]
    }
   ],
   "source": [
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "history = model_comb.fit(x_train_cv, y_train_rnn,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 3s 80ms/step - loss: 0.4985 - accuracy: 0.7978\n",
      "\n",
      "\n",
      "Test score: 0.4984525740146637\n",
      "Test accuracy: 0.797831118106842\n"
     ]
    }
   ],
   "source": [
    "score = model_comb.evaluate(x_valid_cv, y_valid_rnn, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comb2 = Sequential()\n",
    "model_comb2.add(Embedding(input_dim=max_words, output_dim=30, input_length=max_len, trainable=True, mask_zero=True))\n",
    "model_comb2.add(LSTM(128, return_sequences=True))\n",
    "model_comb2.add(Conv1D(128, 3))\n",
    "model_comb2.add(Activation(\"relu\"))\n",
    "model_comb2.add(MaxPooling1D(pool_size=2))\n",
    "model_comb2.add(Conv1D(64, 3))\n",
    "model_comb2.add(Activation(\"relu\"))\n",
    "model_comb2.add(GlobalMaxPool1D())\n",
    "# model_comb.add(Dropout(0.2))\n",
    "model_comb2.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comb2.compile(loss='BinaryCrossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "109/109 [==============================] - 455s 4s/step - loss: 0.4324 - accuracy: 0.8261 - val_loss: 0.3857 - val_accuracy: 0.8381\n",
      "Epoch 2/20\n",
      "109/109 [==============================] - 454s 4s/step - loss: 0.3631 - accuracy: 0.8411 - val_loss: 0.3774 - val_accuracy: 0.8400\n"
     ]
    }
   ],
   "source": [
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "history = model_comb2.fit(x_train_cv, y_train_rnn,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 21s 515ms/step - loss: 0.3694 - accuracy: 0.8490\n",
      "\n",
      "\n",
      "Test score: 0.36941367387771606\n",
      "Test accuracy: 0.8489543199539185\n"
     ]
    }
   ],
   "source": [
    "score = model_comb2.evaluate(x_valid_cv, y_valid_rnn, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comb3 = Sequential()\n",
    "model_comb3.add(Embedding(input_dim=max_words, output_dim=30, input_length=max_len, trainable=True, mask_zero=True))\n",
    "model_comb3.add(LSTM(128, return_sequences=True))\n",
    "model_comb3.add(Conv1D(128, 3))\n",
    "model_comb3.add(Activation(\"relu\"))\n",
    "model_comb3.add(GlobalMaxPool1D())\n",
    "# model_comb.add(Dropout(0.2))\n",
    "model_comb3.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comb3.compile(loss='BinaryCrossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "109/109 [==============================] - 447s 4s/step - loss: 0.4460 - accuracy: 0.8282 - val_loss: 0.4094 - val_accuracy: 0.8290\n",
      "Epoch 2/20\n",
      "109/109 [==============================] - 439s 4s/step - loss: 0.3867 - accuracy: 0.8389 - val_loss: 0.3957 - val_accuracy: 0.8348\n"
     ]
    }
   ],
   "source": [
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "history = model_comb3.fit(x_train_cv, y_train_rnn,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 20s 482ms/step - loss: 0.3841 - accuracy: 0.8373\n",
      "\n",
      "\n",
      "Test score: 0.38406020402908325\n",
      "Test accuracy: 0.8373354077339172\n"
     ]
    }
   ],
   "source": [
    "score = model_comb3.evaluate(x_valid_cv, y_valid_rnn, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—ã–≤–æ–¥—ã:\n",
    "–†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ —Å–µ—Ç–∏ –≤ —Ü–µ–ª–æ–º –ø–æ–∫–∞–∑–∞–ª–∏ —Ö—É–¥—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —á–µ–º —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ. –ö–æ–º–±–∏–Ω–∞—Ü–∏—è CNN->LSTM –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è –≥–æ—Ä–∞–∑–¥–æ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º LSTM/GRU —Å–∞–º–∏ –ø–æ —Å–µ–±–µ, –Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ö–æ–∂–∏–π. –°–∞–º—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—è LSTM->CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
